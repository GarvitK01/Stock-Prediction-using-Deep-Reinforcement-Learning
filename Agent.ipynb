{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35554272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d758e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketAgent():\n",
    "    \n",
    "    def __init__(self, epsilon, gamma, input_dim, states, test = False, model_name = \"Agent\"):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.actions = ['Buy', 'Sell', 'Hold']\n",
    "        self.state_value = np.random.random((states, 1)) \n",
    "        self.policy = np.array((states, 1))\n",
    "        self.model = keras.models.load_model(model_name) if test else self._model\n",
    "        \n",
    "    def take_action(state):\n",
    "        \n",
    "        '''\n",
    "        State in this case will be timestep as it will be easier to index through it\n",
    "        '''\n",
    "        \n",
    "        if random.random() <= epsilon:  # Exploit\n",
    "            return self.actions[np.argmax(self.state_value[state])]\n",
    "        else: # Explore\n",
    "            return random.choice(self.actions)\n",
    "        \n",
    "    def _model():\n",
    "        \n",
    "        '''\n",
    "        Defining the Model that approximates the Value Function\n",
    "        '''\n",
    "        \n",
    "        model = keras.models.Sequential()\n",
    "        model.add(keras.layers.Dense(32, input_dim = (5), activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(64, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(32, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(1, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', metrics = ['accuracy'], optimizer = keras.optimizers.adam_v2.Adam())\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc8e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episode(agent, epsilon, gamma, alpha, data):\n",
    "    \n",
    "    for time in range(1, len(data) - 1):\n",
    "        \n",
    "            state = np.array(data.loc[time])   # State Vector\n",
    "            action = agent.take_action(time)\n",
    "            if action == 'Buy':\n",
    "                \n",
    "            elif action == 'Sell':\n",
    "                \n",
    "                reward = 100 * ((state[3] - data.loc[time-1][3])/data.loc(time-1)[3])\n",
    "                next_state = time + 1\n",
    "                self.state_value[time] += alpha * (reward + gamma * self.state_value[next_state] - self.state_value[time])\n",
    "                done = True if time == len(data) - 2 else False\n",
    "                print(\"********Training Deep Learning Model********\\n\")\n",
    "                agent.model.fit(state, self.state_value[time], epochs = 1, verbose = 1)\n",
    "                \n",
    "            elif action == 'Hold':\n",
    "                \n",
    "                reward = 0\n",
    "                \n",
    "            \n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
